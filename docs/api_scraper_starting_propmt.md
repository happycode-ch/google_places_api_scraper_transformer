Understood. Below is a single, copy-pasteable standalone Markdown document that serves as your Cursor AI Agent Prompt for building a CLI-first Google Places API scraper, with a minimal Streamlit frontend as a second-phase feature.

â¸»


# ğŸ¤– GOOGLE_PLACES_API_SCRAPER.md

## ğŸ§­ Objective

You are an autonomous agent working in Cursor AI. Your task is to fully set up and build a CLI-based Python scraper using the **Google Places API** to collect structured data about **farm shops** in **Canton Aargau, Switzerland**. After the CLI is functional and tested, you will build a **minimal Streamlit frontend** that allows review and basic interaction with the scraped data.

This scraper will support my larger `farmshops.ch` directory project but is designed as a **standalone utility**, housed in:

Project Name: Google Places API Scraper
Project Root: ~/Code/Google_Places_API_Scraper/

The entire stack must remain lean, modular, and human-readable.

---

## ğŸ” Google Cloud Setup â€” CLI Only

You must perform **all setup using the `gcloud` CLI**, not the Google web console. Automate everything.

1. âœ… Check if `gcloud` CLI is installed; install if missing.
2. âœ… Create/select project:
   ```bash
   gcloud projects create farmshops-cli --set-as-default

	3.	âœ… Enable Places API:

gcloud services enable places-backend.googleapis.com


	4.	âœ… Create an API key:

gcloud alpha services api-keys create \
  --display-name="Places API Key for FarmShops" \
  --restrictions=apiTarget=places-backend.googleapis.com


	5.	âœ… Save the key in a .env file:

GOOGLE_API_KEY=your-key-here



â¸»

ğŸ§© Functionality

The scraper must:
	â€¢	Use either:
	â€¢	Text Search: e.g., "farm shops in Aargau Switzerland"
	â€¢	Nearby Search: if a lat/lng grid is needed
	â€¢	Page through all results using next_page_token
	â€¢	Use Place Details API to filter only results in Canton Aargau
	â€¢	Extract structured fields into:
	â€¢	aargau_raw.json
	â€¢	aargau_raw.csv (flattened for Excel)
	â€¢	Optional: aargau_cleaned.json

Fields to extract:

Field	Description
name	Place name
formatted_address	Full address
latitude, longitude	Coordinates
place_id	Google Place identifier
types	Business type (for filtering)
website	Business site (if available)
url	Google Maps link
opening_hours	Human-readable hours
address_components	Used to confirm location is in Aargau

Final output schema (cleaned):

{
  "name": "...",
  "address": "...",
  "latitude": 47.38,
  "longitude": 8.05,
  "products": [],
  "organic_certified": false,
  "payment_methods": [],
  "opening_hours": {
    "Mon": "08:00â€“18:00",
    "Tue": "08:00â€“18:00"
  },
  "website": "https://example.com",
  "google_maps_url": "https://maps.google.com/?q=Hofladen+..."
}


â¸»

ğŸ–¥ï¸ CLI Requirements

Use argparse or click to build a CLI that supports:

python main.py search \
  --region "Aargau" \
  --keyword "farm shops" \
  --mode text \
  --output-json data/aargau_raw.json \
  --output-csv data/aargau_raw.csv

CLI Flags:
	â€¢	--region: Target region, default "Aargau"
	â€¢	--keyword: Default "farm shops"
	â€¢	--mode: "text" or "nearby"
	â€¢	--radius: Optional, for nearby search (in meters)
	â€¢	--output-json: Path to save raw JSON
	â€¢	--output-csv: Path to save a flat CSV for Excel
	â€¢	--clean: Flag to enable schema normalization and output *_cleaned.json

â¸»

ğŸ§± File Structure

Use the following layout:

Google_Places_API_Scraper/
â”œâ”€â”€ main.py              # CLI entry
â”œâ”€â”€ google_places.py     # Search + Place Details
â”œâ”€â”€ cleaner.py           # Normalize to schema
â”œâ”€â”€ geo_utils.py         # Aargau filtering, if needed
â”œâ”€â”€ export.py            # JSON and CSV writers
â”œâ”€â”€ data/                # Output files
â”‚   â”œâ”€â”€ aargau_raw.json
â”‚   â”œâ”€â”€ aargau_raw.csv
â”‚   â””â”€â”€ aargau_cleaned.json
â”œâ”€â”€ streamlit_app.py     # Basic Streamlit frontend (see below)
â””â”€â”€ .env                 # Contains GOOGLE_API_KEY


â¸»

ğŸŒ Streamlit Frontend (Phase 2)

Once the CLI is tested and working, create a minimal streamlit_app.py with the following functionality:
	â€¢	Load either aargau_raw.json or aargau_cleaned.json
	â€¢	Display entries in a table with columns: name, address, lat/lng, website
	â€¢	Add checkbox to filter only entries with websites
	â€¢	Optional: show map with pins (Streamlit + pydeck or folium)

Example command:

streamlit run streamlit_app.py

No admin panel or write-back required â€” just view and filter.

â¸»

ğŸ”’ Environment & Dependencies
	â€¢	Python 3.11+
	â€¢	requests, dotenv, click, pandas, streamlit (install only as needed)
	â€¢	Keep requirements.txt minimal and CLI-first

â¸»

âœ… Completion Criteria

The agent has succeeded when:
	1.	The full CLI scraper runs from main.py and outputs JSON + CSV
	2.	Data is deduplicated and filtered to Aargau
	3.	Output conforms to schema and can be used in farmshops.ch
	4.	Streamlit app launches and shows the results cleanly

â¸»

ğŸ§  Development Philosophy
	â€¢	Zero bloat
	â€¢	CLI-first, Streamlit second
	â€¢	Decoupled output â†’ usable by farmshops.ch or any other project
	â€¢	Everything automatable, nothing locked to a GUI
	â€¢	Built for a human developer to review, not just a bot to run

â¸»

ğŸ’¡ Cursor Agent: Think like an engineer, act like a builder. If youâ€™re unsure about a query limit, deduplication method, or CLI convention, make an intelligent default and document it clearly.

